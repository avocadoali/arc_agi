2024-12-20 09:25:37 PM EST: run from json
2024-12-20 09:25:38 PM EST: process challenges w limit
2024-12-20 09:25:38 PM EST: [00d62c1b] running root node with 5 attempts.
2024-12-20 09:25:38 PM EST: strarting attempt run
2024-12-20 09:25:39 PM EST: Creating vllm client done 
2024-12-20 09:25:39 PM EST: [2N1QQ45E] calling vllm model
2024-12-20 09:25:39 PM EST: Other vllm error: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 4096 tokens. However, you requested 20374 tokens in the messages, Please reduce the length of the messages.", 'type': 'BadRequestError', 'param': None, 'code': 400}, retrying in 0 seconds (0/200)...
2024-12-20 09:25:54 PM EST: [VUCRD1YI] calling vllm model
2024-12-20 09:25:54 PM EST: Other vllm error: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 4096 tokens. However, you requested 20374 tokens in the messages, Please reduce the length of the messages.", 'type': 'BadRequestError', 'param': None, 'code': 400}, retrying in 1 seconds (1/200)...
2024-12-20 09:26:09 PM EST: [XTE1V03Y] calling vllm model
2024-12-20 09:26:09 PM EST: Other vllm error: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 4096 tokens. However, you requested 20374 tokens in the messages, Please reduce the length of the messages.", 'type': 'BadRequestError', 'param': None, 'code': 400}, retrying in 2 seconds (2/200)...
2024-12-20 09:26:24 PM EST: [P6U4VV46] calling vllm model
2024-12-20 09:26:24 PM EST: Other vllm error: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 4096 tokens. However, you requested 20374 tokens in the messages, Please reduce the length of the messages.", 'type': 'BadRequestError', 'param': None, 'code': 400}, retrying in 3 seconds (3/200)...
