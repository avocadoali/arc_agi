2024-12-24 03:02:44 PM EST: run from json
2024-12-24 03:02:44 PM EST: process challenges w limit
2024-12-24 03:02:44 PM EST: [00d62c1b] running root node with 5 attempts.
2024-12-24 03:02:44 PM EST: strarting attempt run
2024-12-24 03:02:46 PM EST: Creating vllm client done 
2024-12-24 03:02:46 PM EST: [31JCON5V] calling vllm model
2024-12-24 03:02:46 PM EST: Other vllm error: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 8192 tokens. However, you requested 23725 tokens (19725 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.", 'type': 'BadRequestError', 'param': None, 'code': 400}, retrying in 0 seconds (0/200)...
2024-12-24 03:03:01 PM EST: [E49L7RPM] calling vllm model
2024-12-24 03:03:01 PM EST: Other vllm error: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 8192 tokens. However, you requested 23725 tokens (19725 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.", 'type': 'BadRequestError', 'param': None, 'code': 400}, retrying in 1 seconds (1/200)...
