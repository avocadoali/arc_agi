{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alfred.nguyen/miniconda3/envs/poetry_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load from env file\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI, or Artificial Intelligence, is a broad field aiming to create machines capable of performing tasks that typically require human intelligence.  While the specific workings vary greatly depending on the type of AI, most approaches involve some combination of the following:\n",
      "\n",
      "**1. Data Acquisition and Preparation:**\n",
      "\n",
      "* **Data Collection:** AI systems learn from data. This data can be anything from text and images to sensor readings and financial transactions.  The more relevant data, the better the AI can perform.\n",
      "* **Data Cleaning:** Raw data is often messy and inconsistent.  AI requires clean, structured data. This involves handling missing values, removing duplicates, and correcting errors.\n",
      "* **Data Preprocessing:** This step transforms the data into a format suitable for the AI algorithm.  This might include scaling numerical values, converting categorical data into numbers, or creating new features from existing ones.\n",
      "\n",
      "**2. Algorithm Selection and Training:**\n",
      "\n",
      "* **Choosing the Right Algorithm:** Different tasks require different algorithms.  Some common types include:\n",
      "    * **Machine Learning (ML):** Algorithms that allow computers to learn from data without explicit programming. Examples include:\n",
      "        * **Supervised Learning:**  Learns from labeled data (input-output pairs). Examples: classification (e.g., spam detection), regression (e.g., predicting house prices).\n",
      "        * **Unsupervised Learning:** Learns from unlabeled data to find patterns and structures. Examples: clustering (e.g., customer segmentation), dimensionality reduction.\n",
      "        * **Reinforcement Learning:**  Learns through trial and error by interacting with an environment and receiving rewards or penalties. Examples: game playing, robotics.\n",
      "    * **Deep Learning (DL):** A subset of ML that uses artificial neural networks with multiple layers to extract higher-level features from data.  Examples: image recognition, natural language processing.\n",
      "    * **Rule-based Systems:**  Use predefined rules to make decisions.  Examples: expert systems, diagnostic systems.\n",
      "* **Training the Model:** The algorithm learns from the prepared data. This involves adjusting internal parameters to minimize errors or maximize rewards.  The process is iterative and often computationally intensive.\n",
      "\n",
      "**3. Evaluation and Deployment:**\n",
      "\n",
      "* **Model Evaluation:** The trained model is tested on unseen data to assess its performance. Metrics like accuracy, precision, and recall are used to measure how well the model generalizes to new data.\n",
      "* **Deployment:** Once the model meets the desired performance criteria, it can be deployed in a real-world application. This could involve integrating it into a software system, embedding it in a device, or making it available through an API.\n",
      "* **Monitoring and Maintenance:** AI models are not static.  Their performance can degrade over time as the underlying data distribution changes.  Continuous monitoring and retraining are essential to ensure ongoing accuracy and effectiveness.\n",
      "\n",
      "**Simplified Analogy:**\n",
      "\n",
      "Imagine training a dog.  You (the programmer) provide the dog (the AI) with examples of good behavior (data) and reward it when it does the right thing (training).  The dog learns from these examples and develops an internal understanding of what's expected (model).  When faced with a new situation, the dog uses this learned knowledge to make a decision (prediction).\n",
      "\n",
      "**Key Concepts:**\n",
      "\n",
      "* **Neural Networks:**  Inspired by the human brain, these interconnected networks of nodes process information through layers, learning complex patterns.\n",
      "* **Big Data:** Large datasets are essential for training effective AI models, particularly in deep learning.\n",
      "* **Cloud Computing:**  Provides the computational power and storage needed for training and deploying complex AI systems.\n",
      "\n",
      "\n",
      "This explanation provides a general overview.  The specifics of how AI works can be quite complex and technical, depending on the chosen approach and the task at hand.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = genai.GenerativeModel(\"gemini-1.5-pro-002\")\n",
    "response = model.generate_content(\"Explain how AI works\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence (AI) aims to mimic human intelligence processes through computer systems. While the field is vast and constantly evolving, most AI systems rely on a few core concepts:\n",
      "\n",
      "**1. Data:**  AI thrives on data.  Large datasets are used to train algorithms, allowing them to learn patterns, make predictions, and perform tasks. The quality, quantity, and diversity of the data are crucial for the effectiveness of the AI system.\n",
      "\n",
      "**2. Algorithms:** These are sets of rules and statistical techniques that guide the AI's learning process.  Different algorithms are suited to different tasks. Common types include:\n",
      "\n",
      "* **Machine Learning (ML):**  Algorithms that allow computers to learn from data without explicit programming.  ML is a subset of AI.\n",
      "    * **Supervised Learning:**  The algorithm learns from labeled data (e.g., images tagged with \"cat\" or \"dog\").\n",
      "    * **Unsupervised Learning:**  The algorithm finds patterns in unlabeled data (e.g., grouping similar customers based on their purchase history).\n",
      "    * **Reinforcement Learning:**  The algorithm learns through trial and error, receiving rewards or penalties for its actions (e.g., a game-playing AI).\n",
      "* **Deep Learning (DL):**  A subset of ML that uses artificial neural networks with multiple layers to analyze complex data. This allows the AI to learn intricate patterns and representations.\n",
      "* **Natural Language Processing (NLP):**  Focuses on enabling computers to understand, interpret, and generate human language.\n",
      "* **Computer Vision:**  Enables computers to \"see\" and interpret images and videos.\n",
      "\n",
      "**3. Models:**  The output of the training process.  A model represents the learned patterns and relationships within the data. It's essentially a mathematical representation of what the AI has learned.  This model is then used to make predictions or perform tasks on new, unseen data.\n",
      "\n",
      "**4. Training:** The process of feeding data to the algorithm so it can learn and create a model. This often involves adjusting parameters within the algorithm to optimize its performance.\n",
      "\n",
      "**5. Inference:**  Once the model is trained, it can be used to make predictions or perform tasks on new data. This is the stage where the AI \"applies\" what it has learned.\n",
      "\n",
      "**Simplified Analogy:**\n",
      "\n",
      "Imagine teaching a child to identify a cat. You show them many pictures of cats, labeling them as \"cat.\"  Eventually, the child learns the characteristics of a cat (furry, four legs, whiskers, etc.). This learning process is analogous to training an AI.  The pictures are the data, the child's brain is the algorithm, and the mental image they form of a cat is the model. When the child sees a new animal, they compare it to their mental model and can identify whether it's a cat or not. This is inference.\n",
      "\n",
      "**Key Considerations:**\n",
      "\n",
      "* **Bias:** AI systems can inherit biases present in the data they are trained on, leading to unfair or discriminatory outcomes.\n",
      "* **Explainability:**  Understanding how some complex AI models arrive at their decisions can be challenging.\n",
      "* **Ethics:**  The use of AI raises ethical concerns around privacy, job displacement, and autonomous weapons systems.\n",
      "\n",
      "This is a simplified overview. The field of AI is incredibly complex and rapidly evolving. New techniques and applications are constantly emerging.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "\n",
    "model = genai.GenerativeModel(\"models/gemini-1.5-pro-002\")\n",
    "response = model.generate_content(\"Explain how AI works\")\n",
    "print(response.text)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(name='models/chat-bison-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='PaLM 2 Chat (Legacy)',\n",
      "      description='A legacy text-only model optimized for chat conversations',\n",
      "      input_token_limit=4096,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
      "      temperature=0.25,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/text-bison-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='PaLM 2 (Legacy)',\n",
      "      description='A legacy model that understands text and generates text as an output',\n",
      "      input_token_limit=8196,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
      "      temperature=0.7,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/embedding-gecko-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding Gecko',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=1024,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-1.0-pro-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Latest',\n",
      "      description=('The original Gemini 1.0 Pro model. This model will be discontinued on '\n",
      "                   'February 15th, 2025. Move to a newer Gemini version.'),\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-1.0-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro',\n",
      "      description='The best model for scaling across a wide range of tasks',\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro',\n",
      "      description='The best model for scaling across a wide range of tasks',\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-1.0-pro-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
      "      description=('The original Gemini 1.0 Pro model version that supports tuning. Gemini 1.0 '\n",
      "                   'Pro will be discontinued on February 15th, 2025. Move to a newer Gemini '\n",
      "                   'version.'),\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
      "      temperature=0.9,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-1.0-pro-vision-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Vision',\n",
      "      description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
      "                   'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
      "                   'Move to a newer Gemini version.'),\n",
      "      input_token_limit=12288,\n",
      "      output_token_limit=4096,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.4,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=32)\n",
      "Model(name='models/gemini-pro-vision',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Vision',\n",
      "      description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
      "                   'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
      "                   'Move to a newer Gemini version.'),\n",
      "      input_token_limit=12288,\n",
      "      output_token_limit=4096,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.4,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=32)\n",
      "Model(name='models/gemini-1.5-pro-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Pro Latest',\n",
      "      description=('Alias that points to the most recent production (non-experimental) release '\n",
      "                   'of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 '\n",
      "                   'million tokens.'),\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-pro-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Pro 001',\n",
      "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
      "                   'supports up to 2 million tokens, released in May of 2024.'),\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-pro-002',\n",
      "      base_model_id='',\n",
      "      version='002',\n",
      "      display_name='Gemini 1.5 Pro 002',\n",
      "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
      "                   'supports up to 2 million tokens, released in September of 2024.'),\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Pro',\n",
      "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
      "                   'supports up to 2 million tokens, released in May of 2024.'),\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-pro-exp-0801',\n",
      "      base_model_id='',\n",
      "      version='exp-0801',\n",
      "      display_name='Gemini 1.5 Pro Experimental 0801',\n",
      "      description=('Experimental release (August 1st, 2024) of Gemini 1.5 Pro, our mid-size '\n",
      "                   'multimodal model that supports up to 2 million tokens, with across the board '\n",
      "                   'improvements. Replaced by Gemini-1.5-pro-002 (stable).'),\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-pro-exp-0827',\n",
      "      base_model_id='',\n",
      "      version='exp-1206',\n",
      "      display_name='Gemini Experimental 1206',\n",
      "      description='Experimental release (December 6th, 2024) of Gemini.',\n",
      "      input_token_limit=2097152,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-flash-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash Latest',\n",
      "      description=('Alias that points to the most recent production (non-experimental) release '\n",
      "                   'of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling '\n",
      "                   'across diverse tasks.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash 001',\n",
      "      description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
      "                   'for scaling across diverse tasks, released in May of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-flash-001-tuning',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash 001 Tuning',\n",
      "      description=('Version of Gemini 1.5 Flash that supports tuning, our fast and versatile '\n",
      "                   'multimodal model for scaling across diverse tasks, released in May of 2024.'),\n",
      "      input_token_limit=16384,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-flash',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash',\n",
      "      description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
      "                   'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-exp-0827',\n",
      "      base_model_id='',\n",
      "      version='exp-1206',\n",
      "      display_name='Gemini Experimental 1206',\n",
      "      description='Experimental release (December 6th, 2024) of Gemini.',\n",
      "      input_token_limit=2097152,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-flash-002',\n",
      "      base_model_id='',\n",
      "      version='002',\n",
      "      display_name='Gemini 1.5 Flash 002',\n",
      "      description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
      "                   'for scaling across diverse tasks, released in September of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash-8B',\n",
      "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
      "                   'Flash model, released in October of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash-8B 001',\n",
      "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
      "                   'Flash model, released in October of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash-8B Latest',\n",
      "      description=('Alias that points to the most recent production (non-experimental) release '\n",
      "                   'of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, '\n",
      "                   'released in October of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
      "      description=('Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our '\n",
      "                   'smallest and most cost effective Flash model. Replaced by '\n",
      "                   'Gemini-1.5-flash-8b-001 (stable).'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b-exp-0924',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash 8B Experimental 0924',\n",
      "      description=('Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our '\n",
      "                   'smallest and most cost effective Flash model. Replaced by '\n",
      "                   'Gemini-1.5-flash-8b-001 (stable).'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash-exp',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash Experimental',\n",
      "      description='Gemini 2.0 Flash Experimental',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-exp-1206',\n",
      "      base_model_id='',\n",
      "      version='exp_1206',\n",
      "      display_name='Gemini Experimental 1206',\n",
      "      description='Experimental release (December 6th, 2024) of Gemini.',\n",
      "      input_token_limit=2097152,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-exp-1121',\n",
      "      base_model_id='',\n",
      "      version='exp-1121',\n",
      "      display_name='Gemini Experimental 1121',\n",
      "      description='Experimental release (November 21st, 2024) of Gemini.',\n",
      "      input_token_limit=32768,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-exp-1114',\n",
      "      base_model_id='',\n",
      "      version='exp-1121',\n",
      "      display_name='Gemini Experimental 1121',\n",
      "      description='Experimental release (November 21st, 2024) of Gemini.',\n",
      "      input_token_limit=32768,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/learnlm-1.5-pro-experimental',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='LearnLM 1.5 Pro Experimental',\n",
      "      description=('Alias that points to the most recent stable version of Gemini 1.5 Pro, our '\n",
      "                   'mid-size multimodal model that supports up to 2 million tokens.'),\n",
      "      input_token_limit=32767,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/embedding-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding 001',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/text-embedding-004',\n",
      "      base_model_id='',\n",
      "      version='004',\n",
      "      display_name='Text Embedding 004',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/aqa',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Model that performs Attributed Question Answering.',\n",
      "      description=('Model trained to return answers to questions that are grounded in provided '\n",
      "                   'sources, along with estimating answerable probability.'),\n",
      "      input_token_limit=7168,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateAnswer'],\n",
      "      temperature=0.2,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=40)\n"
     ]
    }
   ],
   "source": [
    "for x in genai.list_models():\n",
    "    print(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model(name='models/chat-bison-001',\n",
    "      base_model_id='',\n",
    "      version='001',\n",
    "      display_name='PaLM 2 Chat (Legacy)',\n",
    "      description='A legacy text-only model optimized for chat conversations',\n",
    "      input_token_limit=4096,\n",
    "      output_token_limit=1024,\n",
    "      supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
    "      temperature=0.25,\n",
    "      max_temperature=None,\n",
    "      top_p=0.95,\n",
    "      top_k=40)\n",
    "Model(name='models/text-bison-001',\n",
    "      base_model_id='',\n",
    "      version='001',\n",
    "      display_name='PaLM 2 (Legacy)',\n",
    "      description='A legacy model that understands text and generates text as an output',\n",
    "      input_token_limit=8196,\n",
    "      output_token_limit=1024,\n",
    "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
    "      temperature=0.7,\n",
    "      max_temperature=None,\n",
    "      top_p=0.95,\n",
    "      top_k=40)\n",
    "Model(name='models/embedding-gecko-001',\n",
    "      base_model_id='',\n",
    "      version='001',\n",
    "      display_name='Embedding Gecko',\n",
    "      description='Obtain a distributed representation of a text.',\n",
    "      input_token_limit=1024,\n",
    "      output_token_limit=1,\n",
    "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
    "      temperature=None,\n",
    "      max_temperature=None,\n",
    "      top_p=None,\n",
    "      top_k=None)\n",
    "Model(name='models/gemini-1.0-pro-latest',\n",
    "      base_model_id='',\n",
    "      version='001',\n",
    "      display_name='Gemini 1.0 Pro Latest',\n",
    "      description=('The original Gemini 1.0 Pro model. This model will be discontinued on '\n",
    "                   'February 15th, 2025. Move to a newer Gemini version.'),\n",
    "      input_token_limit=30720,\n",
    "      output_token_limit=2048,\n",
    "      supported_generation_methods=['generateContent', 'countTokens'],\n",
    "      temperature=0.9,\n",
    "      max_temperature=None,\n",
    "      top_p=1.0,\n",
    "      top_k=None)\n",
    "Model(name='models/gemini-1.0-pro',\n",
    "      base_model_id='',\n",
    "      version='001',\n",
    "      display_name='Gemini 1.0 Pro',\n",
    "      description='The best model for scaling across a wide range of tasks',\n",
    "      input_token_limit=30720,\n",
    "      output_token_limit=2048,\n",
    "      supported_generation_methods=['generateContent', 'countTokens'],\n",
    "      temperature=0.9,\n",
    "      max_temperature=None,\n",
    "      top_p=1.0,\n",
    "      top_k=None)\n",
    "Model(name='models/gemini-pro',\n",
    "      base_model_id='',\n",
    "      version='001',\n",
    "      display_name='Gemini 1.0 Pro',\n",
    "      description='The best model for scaling across a wide range of tasks',\n",
    "      input_token_limit=30720,\n",
    "      output_token_limit=2048,\n",
    "      supported_generation_methods=['generateContent', 'countTokens'],\n",
    "      temperature=0.9,\n",
    "      max_temperature=None,\n",
    "      top_p=1.0,\n",
    "      top_k=None)\n",
    "Model(name='models/gemini-1.0-pro-001',\n",
    "      base_model_id='',\n",
    "      version='001',\n",
    "      display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
    "      description=('The original Gemini 1.0 Pro model version that supports tuning. Gemini 1.0 '\n",
    "                   'Pro will be discontinued on February 15th, 2025. Move to a newer Gemini '\n",
    "                   'version.'),\n",
    "      input_token_limit=30720,\n",
    "      output_token_limit=2048,\n",
    "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
    "      temperature=0.9,\n",
    "      max_temperature=None,\n",
    "      top_p=1.0,\n",
    "      top_k=None)\n",
    "Model(name='models/gemini-1.0-pro-vision-latest',\n",
    "      base_model_id='',\n",
    "      version='001',\n",
    "      display_name='Gemini 1.0 Pro Vision',\n",
    "      description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
    "                   'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
    "                   'Move to a newer Gemini version.'),\n",
    "      input_token_limit=12288,\n",
    "      output_token_limit=4096,\n",
    "      supported_generation_methods=['generateContent', 'countTokens'],\n",
    "      temperature=0.4,\n",
    "      max_temperature=None,\n",
    "      top_p=1.0,\n",
    "      top_k=32)\n",
    "Model(name='models/gemini-pro-vision',\n",
    "      base_model_id='',\n",
    "      version='001',\n",
    "      display_name='Gemini 1.0 Pro Vision',\n",
    "      description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
    "                   'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
    "                   'Move to a newer Gemini version.'),\n",
    "      input_token_limit=12288,\n",
    "      output_token_limit=4096,\n",
    "      supported_generation_methods=['generateContent', 'countTokens'],\n",
    "      temperature=0.4,\n",
    "      max_temperature=None,\n",
    "      top_p=1.0,\n",
    "      top_k=32)\n",
    "Model(name='models/gemini-1.5-pro-latest',\n",
    "      base_model_id='',\n",
    "      version='001',\n",
    "      display_name='Gemini 1.5 Pro Latest',\n",
    "      description=('Alias that points to the most recent production (non-experimental) release '\n",
    "                   'of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 '\n",
    "                   'million tokens.'),\n",
    "      input_token_limit=2000000,\n",
    "      output_token_limit=8192,\n",
    "      supported_generation_methods=['generateContent', 'countTokens'],\n",
    "      temperature=1.0,\n",
    "      max_temperature=2.0,\n",
    "      top_p=0.95,\n",
    "      top_k=40)\n",
    "Model(name='models/gemini-1.5-pro-001',\n",
    "      base_model_id='',\n",
    "      version='001',\n",
    "      display_name='Gemini 1.5 Pro 001',\n",
    "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
    "                   'supports up to 2 million tokens, released in May of 2024.'),\n",
    "      input_token_limit=2000000,\n",
    "      output_token_limit=8192,\n",
    "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
    "      temperature=1.0,\n",
    "      max_temperature=2.0,\n",
    "      top_p=0.95,\n",
    "      top_k=64)\n",
    "Model(name='models/gemini-1.5-pro-002',\n",
    "      base_model_id='',\n",
    "      version='002',\n",
    "      display_name='Gemini 1.5 Pro 002',\n",
    "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
    "                   'supports up to 2 million tokens, released in September of 2024.'),\n",
    "      input_token_limit=2000000,\n",
    "      output_token_limit=8192,\n",
    "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
    "      temperature=1.0,\n",
    "      max_temperature=2.0,\n",
    "      top_p=0.95,\n",
    "      top_k=40)\n",
    "Model(name='models/gemini-1.5-pro',\n",
    "      base_model_id='',\n",
    "      version='001',\n",
    "      display_name='Gemini 1.5 Pro',\n",
    "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
    "                   'supports up to 2 million tokens, released in May of 2024.'),\n",
    "      input_token_limit=2000000,\n",
    "      output_token_limit=8192,\n",
    "      supported_generation_methods=['generateContent', 'countTokens'],\n",
    "      temperature=1.0,\n",
    "      max_temperature=2.0,\n",
    "      top_p=0.95,\n",
    "      top_k=40)\n",
    "Model(name='models/gemini-1.5-pro-exp-0801',\n",
    "      base_model_id='',\n",
    "      version='exp-0801',\n",
    "      display_name='Gemini 1.5 Pro Experimental 0801',\n",
    "      description=('Experimental release (August 1st, 2024) of Gemini 1.5 Pro, our mid-size '\n",
    "                   'multimodal model that supports up to 2 million tokens, with across the board '\n",
    "                   'improvements. Replaced by Gemini-1.5-pro-002 (stable).'),\n",
    "      input_token_limit=2000000,\n",
    "      output_token_limit=8192,\n",
    "      supported_generation_methods=['generateContent', 'countTokens'],\n",
    "      temperature=1.0,\n",
    "      max_temperature=2.0,\n",
    "      top_p=0.95,\n",
    "      top_k=64)\n",
    "Model(name='models/gemini-1.5-pro-exp-0827',\n",
    "      base_model_id='',\n",
    "      version='exp-1206',\n",
    "      display_name='Gemini Experimental 1206',\n",
    "      description='Experimental release (December 6th, 2024) of Gemini.',\n",
    "      input_token_limit=2097152,\n",
    "      output_token_limit=8192,\n",
    "      supported_generation_methods=['generateContent', 'countTokens'],\n",
    "      temperature=1.0,\n",
    "      max_temperature=2.0,\n",
    "      top_p=0.95,\n",
    "      top_k=64)\n",
    "Model(name='models/gemini-1.5-flash-latest',\n",
    "      base_model_id='',\n",
    "      version='001',\n",
    "      display_name='Gemini 1.5 Flash Latest',\n",
    "      description=('Alias that points to the most recent production (non-experimental) release '\n",
    "                   'of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling '\n",
    "                   'across diverse tasks.'),\n",
    "      input_token_limit=1000000,\n",
    "      output_token_limit=8192,\n",
    "      supported_generation_methods=['generateContent', 'countTokens'],\n",
    "      temperature=1.0,\n",
    "      max_temperature=2.0,\n",
    "      top_p=0.95,\n",
    "      top_k=40)\n",
    "Model(name='models/gemini-1.5-flash-001',\n",
    "      base_model_id='',\n",
    "      version='001',\n",
    "      display_name='Gemini 1.5 Flash 001',\n",
    "      description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
    "                   'for scaling across diverse tasks, released in May of 2024.'),\n",
    "      input_token_limit=1000000,\n",
    "      output_token_limit=8192,\n",
    "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
    "      temperature=1.0,\n",
    "      max_temperature=2.0,\n",
    "      top_p=0.95,\n",
    "      top_k=64)\n",
    "Model(name='models/gemini-1.5-flash-001-tuning',\n",
    "      base_model_id='',\n",
    "      version='001',\n",
    "      display_name='Gemini 1.5 Flash 001 Tuning',\n",
    "      description=('Version of Gemini 1.5 Flash that supports tuning, our fast and versatile '\n",
    "                   'multimodal model for scaling across diverse tasks, released in May of 2024.'),\n",
    "      input_token_limit=16384,\n",
    "      output_token_limit=8192,\n",
    "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
    "      temperature=1.0,\n",
    "      max_temperature=2.0,\n",
    "      top_p=0.95,\n",
    "      top_k=64)\n",
    "Model(name='models/gemini-1.5-flash',\n",
    "      base_model_id='',\n",
    "      version='001',\n",
    "      display_name='Gemini 1.5 Flash',\n",
    "      description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
    "                   'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
    "      input_token_limit=1000000,\n",
    "      output_token_limit=8192,\n",
    "      supported_generation_methods=['generateContent', 'countTokens'],\n",
    "      temperature=1.0,\n",
    "      max_temperature=2.0,\n",
    "      top_p=0.95,\n",
    "      top_k=40)\n",
    "Model(name='models/gemini-1.5-flash-exp-0827',\n",
    "      base_model_id='',\n",
    "      version='exp-1206',\n",
    "      display_name='Gemini Experimental 1206',\n",
    "      description='Experimental release (December 6th, 2024) of Gemini.',\n",
    "      input_token_limit=2097152,\n",
    "      output_token_limit=8192,\n",
    "      supported_generation_methods=['generateContent', 'countTokens'],\n",
    "      temperature=1.0,\n",
    "      max_temperature=2.0,\n",
    "      top_p=0.95,\n",
    "      top_k=64)\n",
    "Model(name='models/gemini-1.5-flash-002',\n",
    "      base_model_id='',\n",
    "      version='002',\n",
    "      display_name='Gemini 1.5 Flash 002',\n",
    "      description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
    "                   'for scaling across diverse tasks, released in September of 2024.'),\n",
    "      input_token_limit=1000000,\n",
    "      output_token_limit=8192,\n",
    "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
    "      temperature=1.0,\n",
    "      max_temperature=2.0,\n",
    "      top_p=0.95,\n",
    "      top_k=40)\n",
    "Model(name='models/gemini-1.5-flash-8b',\n",
    "      base_model_id='',\n",
    "      version='001',\n",
    "      display_name='Gemini 1.5 Flash-8B',\n",
    "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
    "                   'Flash model, released in October of 2024.'),\n",
    "      input_token_limit=1000000,\n",
    "      output_token_limit=8192,\n",
    "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
    "      temperature=1.0,\n",
    "      max_temperature=2.0,\n",
    "      top_p=0.95,\n",
    "      top_k=40)\n",
    "Model(name='models/gemini-1.5-flash-8b-001',\n",
    "      base_model_id='',\n",
    "      version='001',\n",
    "      display_name='Gemini 1.5 Flash-8B 001',\n",
    "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
    "                   'Flash model, released in October of 2024.'),\n",
    "      input_token_limit=1000000,\n",
    "      output_token_limit=8192,\n",
    "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
    "      temperature=1.0,\n",
    "      max_temperature=2.0,\n",
    "      top_p=0.95,\n",
    "      top_k=40)\n",
    "Model(name='models/gemini-1.5-flash-8b-latest',\n",
    "      base_model_id='',\n",
    "      version='001',\n",
    "      display_name='Gemini 1.5 Flash-8B Latest',\n",
    "      description=('Alias that points to the most recent production (non-experimental) release '\n",
    "                   'of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, '\n",
    "                   'released in October of 2024.'),\n",
    "      input_token_limit=1000000,\n",
    "      output_token_limit=8192,\n",
    "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
    "      temperature=1.0,\n",
    "      max_temperature=2.0,\n",
    "      top_p=0.95,\n",
    "      top_k=40)\n",
    "Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
    "      base_model_id='',\n",
    "      version='001',\n",
    "      display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
    "      description=('Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our '\n",
    "                   'smallest and most cost effective Flash model. Replaced by '\n",
    "                   'Gemini-1.5-flash-8b-001 (stable).'),\n",
    "      input_token_limit=1000000,\n",
    "      output_token_limit=8192,\n",
    "      supported_generation_methods=['generateContent', 'countTokens'],\n",
    "      temperature=1.0,\n",
    "      max_temperature=2.0,\n",
    "      top_p=0.95,\n",
    "      top_k=40)\n",
    "Model(name='models/gemini-1.5-flash-8b-exp-0924',\n",
    "      base_model_id='',\n",
    "      version='001',\n",
    "      display_name='Gemini 1.5 Flash 8B Experimental 0924',\n",
    "      description=('Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our '\n",
    "                   'smallest and most cost effective Flash model. Replaced by '\n",
    "                   'Gemini-1.5-flash-8b-001 (stable).'),\n",
    "      input_token_limit=1000000,\n",
    "      output_token_limit=8192,\n",
    "      supported_generation_methods=['generateContent', 'countTokens'],\n",
    "      temperature=1.0,\n",
    "      max_temperature=2.0,\n",
    "      top_p=0.95,\n",
    "      top_k=40)\n",
    "Model(name='models/gemini-2.0-flash-exp',\n",
    "      base_model_id='',\n",
    "      version='2.0',\n",
    "      display_name='Gemini 2.0 Flash Experimental',\n",
    "      description='Gemini 2.0 Flash Experimental',\n",
    "      input_token_limit=1048576,\n",
    "      output_token_limit=8192,\n",
    "      supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
    "      temperature=1.0,\n",
    "      max_temperature=2.0,\n",
    "      top_p=0.95,\n",
    "      top_k=40)\n",
    "Model(name='models/gemini-exp-1206',\n",
    "      base_model_id='',\n",
    "      version='exp_1206',\n",
    "      display_name='Gemini Experimental 1206',\n",
    "      description='Experimental release (December 6th, 2024) of Gemini.',\n",
    "      input_token_limit=2097152,\n",
    "      output_token_limit=8192,\n",
    "      supported_generation_methods=['generateContent', 'countTokens'],\n",
    "      temperature=1.0,\n",
    "      max_temperature=2.0,\n",
    "      top_p=0.95,\n",
    "      top_k=64)\n",
    "Model(name='models/gemini-exp-1121',\n",
    "      base_model_id='',\n",
    "      version='exp-1121',\n",
    "      display_name='Gemini Experimental 1121',\n",
    "      description='Experimental release (November 21st, 2024) of Gemini.',\n",
    "      input_token_limit=32768,\n",
    "      output_token_limit=8192,\n",
    "      supported_generation_methods=['generateContent', 'countTokens'],\n",
    "      temperature=1.0,\n",
    "      max_temperature=2.0,\n",
    "      top_p=0.95,\n",
    "      top_k=64)\n",
    "Model(name='models/gemini-exp-1114',\n",
    "      base_model_id='',\n",
    "      version='exp-1121',\n",
    "      display_name='Gemini Experimental 1121',\n",
    "      description='Experimental release (November 21st, 2024) of Gemini.',\n",
    "      input_token_limit=32768,\n",
    "      output_token_limit=8192,\n",
    "      supported_generation_methods=['generateContent', 'countTokens'],\n",
    "      temperature=1.0,\n",
    "      max_temperature=2.0,\n",
    "      top_p=0.95,\n",
    "      top_k=64)\n",
    "Model(name='models/learnlm-1.5-pro-experimental',\n",
    "      base_model_id='',\n",
    "      version='001',\n",
    "      display_name='LearnLM 1.5 Pro Experimental',\n",
    "      description=('Alias that points to the most recent stable version of Gemini 1.5 Pro, our '\n",
    "                   'mid-size multimodal model that supports up to 2 million tokens.'),\n",
    "      input_token_limit=32767,\n",
    "      output_token_limit=8192,\n",
    "      supported_generation_methods=['generateContent', 'countTokens'],\n",
    "      temperature=1.0,\n",
    "      max_temperature=2.0,\n",
    "      top_p=0.95,\n",
    "      top_k=64)\n",
    "Model(name='models/embedding-001',\n",
    "      base_model_id='',\n",
    "      version='001',\n",
    "      display_name='Embedding 001',\n",
    "      description='Obtain a distributed representation of a text.',\n",
    "      input_token_limit=2048,\n",
    "      output_token_limit=1,\n",
    "      supported_generation_methods=['embedContent'],\n",
    "      temperature=None,\n",
    "      max_temperature=None,\n",
    "      top_p=None,\n",
    "      top_k=None)\n",
    "Model(name='models/text-embedding-004',\n",
    "      base_model_id='',\n",
    "      version='004',\n",
    "      display_name='Text Embedding 004',\n",
    "      description='Obtain a distributed representation of a text.',\n",
    "      input_token_limit=2048,\n",
    "      output_token_limit=1,\n",
    "      supported_generation_methods=['embedContent'],\n",
    "      temperature=None,\n",
    "      max_temperature=None,\n",
    "      top_p=None,\n",
    "      top_k=None)\n",
    "Model(name='models/aqa',\n",
    "      base_model_id='',\n",
    "      version='001',\n",
    "      display_name='Model that performs Attributed Question Answering.',\n",
    "      description=('Model trained to return answers to questions that are grounded in provided '\n",
    "                   'sources, along with estimating answerable probability.'),\n",
    "      input_token_limit=7168,\n",
    "      output_token_limit=1024,\n",
    "      supported_generation_methods=['generateAnswer'],\n",
    "      temperature=0.2,\n",
    "      max_temperature=None,\n",
    "      top_p=1.0,\n",
    "      top_k=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing logs\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "# import attrs\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from src.prompts.colors import color_map\n",
    "\n",
    "invalid_color = (255, 255, 255)  # White\n",
    "\n",
    "edge_color = (85, 85, 85)  # Grey edge color\n",
    "white = (255, 255, 255)  # White\n",
    "\n",
    "highlight_color = (255, 0, 0)  # Red\n",
    "\n",
    "\n",
    "# @attrs.frozen\n",
    "class RenderArgs:\n",
    "    cell_size: int = 40\n",
    "    use_border: bool = False\n",
    "    use_larger_edges: bool = True\n",
    "    use_alt_color_scheme: bool = False\n",
    "    force_high_res: bool = False\n",
    "    force_edge_size: int | None = None\n",
    "    lower_cell_size_on_bigger_to: int | None = None\n",
    "    # avoid_edge_around_border: bool = False\n",
    "\n",
    "\n",
    "def create_rgb_grid(\n",
    "    grid: np.ndarray,\n",
    "    render_args: RenderArgs = RenderArgs(),\n",
    "    should_highlight: np.ndarray | None = None,\n",
    "    lower_right_triangle: np.ndarray | None = None,\n",
    "):\n",
    "    # this_color_scheme = alt_color_scheme\n",
    "    # this_color_scheme = (\n",
    "    #     alt_color_scheme if render_args.use_alt_color_scheme else color_scheme\n",
    "    # )\n",
    "\n",
    "    height, width = grid.shape\n",
    "\n",
    "    cell_size = render_args.cell_size\n",
    "    use_border = render_args.use_border\n",
    "    use_larger_edges = render_args.use_larger_edges\n",
    "    force_edge_size = render_args.force_edge_size\n",
    "    # avoid_edge_around_border = render_args.avoid_edge_around_border\n",
    "\n",
    "    if render_args.lower_cell_size_on_bigger_to is not None and (\n",
    "        height > 10 or width > 10\n",
    "    ):\n",
    "        cell_size = render_args.lower_cell_size_on_bigger_to\n",
    "\n",
    "    if force_edge_size is not None:\n",
    "        edge_size = force_edge_size\n",
    "    else:\n",
    "        edge_size = max(cell_size // 8, 1) if use_larger_edges else 1\n",
    "\n",
    "    # Calculate the size of the new grid with edges\n",
    "    new_height = height * (cell_size + edge_size) + edge_size\n",
    "    new_width = width * (cell_size + edge_size) + edge_size\n",
    "\n",
    "    # Create a new grid filled with the edge color\n",
    "    rgb_grid = np.full((new_height, new_width, 3), edge_color, dtype=np.uint8)\n",
    "\n",
    "    # Fill in the cells with the appropriate colors\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            color = color_map[grid[i, j]].rgb\n",
    "            start_row = i * (cell_size + edge_size) + edge_size\n",
    "            start_col = j * (cell_size + edge_size) + edge_size\n",
    "\n",
    "            if should_highlight is not None and should_highlight[i, j]:\n",
    "                rgb_grid[\n",
    "                    start_row : start_row + cell_size, start_col : start_col + cell_size\n",
    "                ] = highlight_color\n",
    "                highlight_width = cell_size // 8\n",
    "                rgb_grid[\n",
    "                    start_row + highlight_width : start_row\n",
    "                    + cell_size\n",
    "                    - highlight_width,\n",
    "                    start_col + highlight_width : start_col\n",
    "                    + cell_size\n",
    "                    - highlight_width,\n",
    "                ] = color\n",
    "\n",
    "                assert (\n",
    "                    lower_right_triangle is None\n",
    "                ), \"Can't highlight and lower right triangle at the same time (yet)\"\n",
    "\n",
    "            else:\n",
    "                rgb_grid[\n",
    "                    start_row : start_row + cell_size, start_col : start_col + cell_size\n",
    "                ] = color\n",
    "\n",
    "                if lower_right_triangle is not None:\n",
    "                    lower_right_triangle_color = color_map[\n",
    "                        lower_right_triangle[i, j]\n",
    "                    ].rgb\n",
    "                    for r in range(cell_size):\n",
    "                        for c in range(cell_size):\n",
    "                            if r > c:\n",
    "                                rgb_grid[\n",
    "                                    start_row + r, start_col + cell_size - 1 - c\n",
    "                                ] = lower_right_triangle_color\n",
    "\n",
    "    # if avoid_edge_around_border:\n",
    "    #     return rgb_grid[\n",
    "    #         edge_size : new_height - edge_size, edge_size : new_width - edge_size\n",
    "    #     ]\n",
    "\n",
    "    if not use_border:\n",
    "        return rgb_grid\n",
    "\n",
    "    rgb_grid_border = np.full(\n",
    "        (new_height + cell_size, new_width + cell_size, 3), white, dtype=np.uint8\n",
    "    )\n",
    "    assert cell_size % 2 == 0\n",
    "    rgb_grid_border[\n",
    "        cell_size // 2 : new_height + cell_size // 2,\n",
    "        cell_size // 2 : new_width + cell_size // 2,\n",
    "    ] = rgb_grid\n",
    "\n",
    "    return rgb_grid_border\n",
    "\n",
    "\n",
    "def grid_to_pil(\n",
    "    grid: np.ndarray,\n",
    "    render_args: RenderArgs = RenderArgs(),\n",
    "    should_highlight: np.ndarray | None = None,\n",
    "    lower_right_triangle: np.ndarray | None = None,\n",
    "):\n",
    "    rgb_grid = create_rgb_grid(\n",
    "        grid,\n",
    "        render_args=render_args,\n",
    "        should_highlight=should_highlight,\n",
    "        lower_right_triangle=lower_right_triangle,\n",
    "    )\n",
    "    return Image.fromarray(rgb_grid, \"RGB\")\n",
    "\n",
    "\n",
    "def grid_to_base64_png(\n",
    "    grid: np.ndarray,\n",
    "    render_args: RenderArgs = RenderArgs(),\n",
    "    should_highlight: np.ndarray | None = None,\n",
    "    lower_right_triangle: np.ndarray | None = None,\n",
    ") -> str:\n",
    "    image = grid_to_pil(\n",
    "        grid,\n",
    "        render_args=render_args,\n",
    "        should_highlight=should_highlight,\n",
    "        lower_right_triangle=lower_right_triangle,\n",
    "    )\n",
    "\n",
    "    output = BytesIO()\n",
    "    image.save(output, format=\"PNG\")\n",
    "    return base64.b64encode(output.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def grid_to_base64_png_oai_content(\n",
    "    grid: np.ndarray,\n",
    "    render_args: RenderArgs = RenderArgs(),\n",
    "    should_highlight: np.ndarray | None = None,\n",
    "    lower_right_triangle: np.ndarray | None = None,\n",
    ") -> dict[str, str]:\n",
    "    base64_png = grid_to_base64_png(\n",
    "        grid,\n",
    "        render_args=render_args,\n",
    "        should_highlight=should_highlight,\n",
    "        lower_right_triangle=lower_right_triangle,\n",
    "    )\n",
    "\n",
    "    # rgb_grid_for_shape = create_rgb_grid(\n",
    "    #     grid,\n",
    "    #     render_args=render_args,\n",
    "    #     should_highlight=should_highlight,\n",
    "    #     lower_right_triangle=lower_right_triangle,\n",
    "    # )\n",
    "\n",
    "    extra = {\"detail\": \"high\"} if render_args.force_high_res else {}\n",
    "\n",
    "    # print(f\"{rgb_grid_for_shape.shape=}\")\n",
    "\n",
    "    # NOTE: we currently use \"auto\". Seems fine for now I think...\n",
    "    return {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\n",
    "            \"url\": f\"data:image/png;base64,{base64_png}\",\n",
    "            **extra,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def show_grid(\n",
    "    grid: np.ndarray,\n",
    "    render_args: RenderArgs = RenderArgs(),\n",
    "    should_highlight: np.ndarray | None = None,\n",
    "    lower_right_triangle: np.ndarray | None = None,\n",
    "):\n",
    "    grid_to_pil(\n",
    "        grid,\n",
    "        render_args=render_args,\n",
    "        should_highlight=should_highlight,\n",
    "        lower_right_triangle=lower_right_triangle,\n",
    "    ).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x127a03070>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Example usage\n",
    "initial_values = np.array([[1, 1, 2], [2, 3, 5], [0, 2, 1]])\n",
    "\n",
    "rgb_grid = create_rgb_grid(\n",
    "    initial_values,\n",
    "    # cell_size=10,\n",
    ")\n",
    "\n",
    "# image = Image.fromarray(rgb_grid, \"RGB\")\n",
    "# image.show()\n",
    "\n",
    "# visualize image in jupyter notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(rgb_grid)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
